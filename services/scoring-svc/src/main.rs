// services/scoring-svc/src/main.rs

// Import necessary libraries from the tonic and tokio crates.
use tonic::{transport::Server, Request, Response, Status};
use std::sync::Arc;
use tokio::sync::Mutex;

// This line includes the Rust code generated by tonic-build from our .proto file.
// The code will be placed in the `scoring` module.
pub mod scoring {
    tonic::include_proto!("scoring.v1");
}

// Import the generated structs and server traits.
use scoring::{
    scoring_service_server::{ScoringService, ScoringServiceServer},
    PredictRequest, PredictResponse,
};

// --- Mock ML Model ---
// This struct is a placeholder for a real machine learning model.
// In a production system, this would load a model file (e.g., ONNX, TensorRT)
// and use a proper inference engine.
struct Model {
    version: String,
}

impl Model {
    fn new() -> Self {
        // In a real app, this would load the latest model from a registry
        // or a shared volume populated by the FL coordinator.
        Model {
            version: "v1.0.0-mock".to_string(),
        }
    }

    // This function simulates running inference.
    fn predict(&self, _request: &PredictRequest) -> f32 {
        // The prediction logic would go here.
        // For now, we'll return a deterministic mock score.
        // A real model would analyze the features in the request.
        0.987
    }
}

// --- gRPC Service Implementation ---

// This struct holds the state for our service, such as a handle to the ML model.
// Using Arc<Mutex<>> allows safe, shared access to the model across multiple
// concurrent gRPC requests.
pub struct MyScoringService {
    model: Arc<Mutex<Model>>,
}

// This block implements the `ScoringService` trait defined in our proto file.
#[tonic::async_trait]
impl ScoringService for MyScoringService {
    /// The `predict` function is called for each incoming gRPC request.
    async fn predict(
        &self,
        request: Request<PredictRequest>,
    ) -> Result<Response<PredictResponse>, Status> {
        let req_features = request.into_inner();
        println!("Received scoring request for transaction: {}", req_features.transaction_id);

        // Lock the model for safe access.
        let model = self.model.lock().await;

        // Run inference.
        let score = model.predict(&req_features);

        // TODO: Write the score and features to a Cassandra table for auditing
        // and later analysis.

        // Create the response payload.
        let reply = PredictResponse {
            risk_score: score,
            model_version: model.version.clone(),
        };

        Ok(Response::new(reply))
    }
}

// --- Server Entrypoint ---

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let addr = "[::]:50051".parse()?;
    println!("ðŸŽ¯ Scoring Service listening on {}", addr);

    // Initialize our service state.
    let model = Arc::new(Mutex::new(Model::new()));
    let scorer = MyScoringService { model };

    // Build and run the gRPC server.
    Server::builder()
        .add_service(ScoringServiceServer::new(scorer))
        .serve(addr)
        .await?;

    Ok(())
}
